{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"# \u26a1 Finetune  **20+ high-performance LLMs with recipes to pretrain, finetune, and deploy at scale.**  <pre>\n\u2705 From scratch implementations     \u2705 No abstractions    \u2705 Beginner friendly\n\u2705 Flash attention                  \u2705 FSDP               \u2705 LoRA, QLoRA, Adapter\n\u2705 Reduce GPU memory (fp4/8/16/32)  \u2705 1-1000+ GPUs/TPUs  \u2705 20+ LLMs\n</pre>  ______________________________________________________________________  [![python](https://img.shields.io/badge/-Python_3.8_%7C_3.9_%7C_3.10-blue?logo=python&amp;logoColor=white)](https://github.com/pre-commit/pre-commit) [![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&amp;logoColor=white)](https://pytorch.org/get-started/locally/) [![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&amp;logoColor=white)](https://pytorchlightning.ai/) [![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) [![tests](https://github.com/Mai0313/finetune/actions/workflows/test.yml/badge.svg)](https://github.com/Mai0313/finetune/actions/workflows/test.yml) [![code-quality](https://github.com/Mai0313/finetune/actions/workflows/code-quality-check.yml/badge.svg)](https://github.com/Mai0313/finetune/actions/workflows/code-quality-check.yml) [![codecov](https://codecov.io/gh/Mai0313/finetune/branch/master/graph/badge.svg)](https://codecov.io/gh/Mai0313/finetune) [![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/Mai0313/finetune/tree/master?tab=License-1-ov-file) [![PRs](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/Mai0313/finetune/pulls) [![contributors](https://img.shields.io/github/contributors/Mai0313/finetune.svg)](https://github.com/Mai0313/finetune/graphs/contributors)  <p> Quick start \u2022   Models \u2022   Finetune \u2022   Deploy \u2022   All workflows \u2022   Features \u2022   Recipes (YAML) \u2022   Lightning AI \u2022     Tutorials </p>"},{"location":"#use-finetune-pretrain-and-deploy-llms-lightning-fast","title":"Use, finetune, pretrain, and deploy LLMs Lightning fast \u26a1\u26a1","text":"<p>Every LLM is implemented from scratch with no abstractions and full control, making them blazing fast, minimal, and performant at enterprise scale.</p> <p>\u2705 Enterprise ready - Apache 2.0 for unlimited enterprise use. \u2705 Developer friendly - Easy debugging with no abstraction layers and single file implementations. \u2705 Optimized performance - Models designed to maximize performance, reduce costs, and speed up training. \u2705 Proven recipes - Highly-optimized training/finetuning recipes tested at enterprise scale.</p> <p> </p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Install LitGPT</p> <pre><code>pip install 'litgpt[all]'\n</code></pre> <p>Load and use any of the 20+ LLMs:</p> <pre><code>from litgpt import LLM\n\nllm = LLM.load(\"microsoft/phi-2\")\ntext = llm.generate(\"Fix the spelling: Every fall, the family goes to the mountains.\")\nprint(text)\n# Corrected Sentence: Every fall, the family goes to the mountains.\n</code></pre> <p> </p> <p>\u2705 Optimized for fast inference \u2705 Quantization \u2705 Runs on low-memory GPUs \u2705 No layers of internal abstractions \u2705 Optimized for production scale</p> Advanced install options  Install from source:  <pre><code>git clone https://github.com/Lightning-AI/litgpt\ncd litgpt\npip install -e '.[all]'\n</code></pre> <p>Explore the full Python API docs.</p> <p> </p>"},{"location":"#choose-from-20-llms","title":"Choose from 20+ LLMs","text":"<p>Every model is written from scratch to maximize performance and remove layers of abstraction:</p> Model Model size Author Reference Llama 3, 3.1, 3.2 1B, 3B, 8B, 70B, 405B Meta AI Meta AI 2024 Code Llama 7B, 13B, 34B, 70B Meta AI Rozi\u00e8re et al. 2023 Mixtral MoE 8x7B, 8x22B Mistral AI Mistral AI 2023 Mistral 7B, 123B Mistral AI Mistral AI 2023 CodeGemma 7B Google Google Team, Google Deepmind Gemma 2 2B, 9B, 27B Google Google Team, Google Deepmind Phi 3 &amp; 3.5 3.8B Microsoft Abdin et al. 2024 ... ... ... ... See full list of 20+ LLMs  \u00a0  #### All models  | Model                        | Model size                               | Author                          | Reference                                                                                              | | ---------------------------- | ---------------------------------------- | ------------------------------- | ------------------------------------------------------------------------------------------------------ | | CodeGemma                    | 7B                                       | Google                          | [Google Team, Google Deepmind](https://ai.google.dev/gemma/docs/codegemma)                             | | Code Llama                   | 7B, 13B, 34B, 70B                        | Meta AI                         | [Rozi\u00e8re et al. 2023](https://arxiv.org/abs/2308.12950)                                                | | Falcon                       | 7B, 40B, 180B                            | TII UAE                         | [TII 2023](https://falconllm.tii.ae)                                                                   | | FreeWilly2 (Stable Beluga 2) | 70B                                      | Stability AI                    | [Stability AI 2023](https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models)       | | Function Calling Llama 2     | 7B                                       | Trelis                          | [Trelis et al. 2023](https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2)             | | Gemma                        | 2B, 7B                                   | Google                          | [Google Team, Google Deepmind](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)   | | Gemma 2                      | 9B, 27B                                  | Google                          | [Google Team, Google Deepmind](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf) | | Llama 2                      | 7B, 13B, 70B                             | Meta AI                         | [Touvron et al. 2023](https://arxiv.org/abs/2307.09288)                                                | | Llama 3.1                    | 8B, 70B                                  | Meta AI                         | [Meta AI 2024](https://github.com/meta-llama/llama3)                                                   | | Llama 3.2                    | 1B, 3B                                   | Meta AI                         | [Meta AI 2024](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)            | | Llama 3.3                    | 70B                                      | Meta AI                         | [Meta AI 2024](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)                               | | Mathstral                    | 7B                                       | Mistral AI                      | [Mistral AI 2024](https://mistral.ai/news/mathstral/)                                                  | | MicroLlama                   | 300M                                     | Ken Wang                        | [MicroLlama repo](https://github.com/keeeeenw/MicroLlama)                                              | | Mixtral MoE                  | 8x7B                                     | Mistral AI                      | [Mistral AI 2023](https://mistral.ai/news/mixtral-of-experts/)                                         | | Mistral                      | 7B, 123B                                 | Mistral AI                      | [Mistral AI 2023](https://mistral.ai/news/announcing-mistral-7b/)                                      | | Mixtral MoE                  | 8x22B                                    | Mistral AI                      | [Mistral AI 2024](https://mistral.ai/news/mixtral-8x22b/)                                              | | OLMo                         | 1B, 7B                                   | Allen Institute for AI (AI2)    | [Groeneveld et al. 2024](https://aclanthology.org/2024.acl-long.841/)                                  | | OpenLLaMA                    | 3B, 7B, 13B                              | OpenLM Research                 | [Geng &amp; Liu 2023](https://github.com/openlm-research/open_llama)                                       | | Phi 1.5 &amp; 2                  | 1.3B, 2.7B                               | Microsoft Research              | [Li et al. 2023](https://arxiv.org/abs/2309.05463)                                                     | | Phi 3                        | 3.8B                                     | Microsoft Research              | [Abdin et al. 2024](https://arxiv.org/abs/2404.14219)                                                  | | Platypus                     | 7B, 13B, 70B                             | Lee et al.                      | [Lee, Hunter, and Ruiz 2023](https://arxiv.org/abs/2308.07317)                                         | | Pythia                       | {14,31,70,160,410}M, {1,1.4,2.8,6.9,12}B | EleutherAI                      | [Biderman et al. 2023](https://arxiv.org/abs/2304.01373)                                               | | Qwen2.5                      | 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B        | Alibaba Group                   | [Qwen Team 2024](https://qwenlm.github.io/blog/qwen2.5/)                                               | | Qwen2.5 Coder                | 0.5B, 1.5B, 3B, 7B, 14B, 32B             | Alibaba Group                   | [Hui, Binyuan et al. 2024](https://arxiv.org/abs/2409.12186)                                           | | Qwen2.5 Math                 | 1.5B, 7B, 72B                            | Alibaba Group                   | [An, Yang et al. 2024](https://arxiv.org/abs/2409.12122)                                               | | QwQ                          | 32B                                      | Alibaba Group                   | [Qwen Team 2024](https://qwenlm.github.io/blog/qwq-32b-preview/)                                       | | SmolLM2                      | 135M, 360M, 1.7B                         | Hugging Face                    | [Hugging Face 2024](https://github.com/huggingface/smollm)                                             | | Salamandra                   | 2B, 7B                                   | Barcelona Supercomputing Centre | [BSC-LTC 2024](https://github.com/BSC-LTC/salamandra)                                                  | | StableCode                   | 3B                                       | Stability AI                    | [Stability AI 2023](https://stability.ai/blog/stablecode-llm-generative-ai-coding)                     | | StableLM                     | 3B, 7B                                   | Stability AI                    | [Stability AI 2023](https://github.com/Stability-AI/StableLM)                                          | | StableLM Zephyr              | 3B                                       | Stability AI                    | [Stability AI 2023](https://stability.ai/blog/stablecode-llm-generative-ai-coding)                     | | TinyLlama                    | 1.1B                                     | Zhang et al.                    | [Zhang et al. 2023](https://github.com/jzhang38/TinyLlama)                                             |  **Tip**: You can list all available models by running the `litgpt download list` command.   <p> </p>"},{"location":"#workflows","title":"Workflows","text":"<p> Finetune \u2022   Pretrain \u2022   Continued pretraining \u2022     Evaluate \u2022     Deploy \u2022     Test </p> <p> </p> <p>Use the command line interface to run advanced workflows such as pretraining or finetuning on your own data.</p>"},{"location":"#all-workflows","title":"All workflows","text":"<p>After installing LitGPT, select the model and workflow to run (finetune, pretrain, evaluate, deploy, etc...):</p> <pre><code># ligpt [action] [model]\nlitgpt  serve     meta-llama/Llama-3.2-3B-Instruct\nlitgpt  finetune  meta-llama/Llama-3.2-3B-Instruct\nlitgpt  pretrain  meta-llama/Llama-3.2-3B-Instruct\nlitgpt  chat      meta-llama/Llama-3.2-3B-Instruct\nlitgpt  evaluate  meta-llama/Llama-3.2-3B-Instruct\n</code></pre> <p> </p>"},{"location":"#finetune-an-llm","title":"Finetune an LLM","text":"<p>Finetuning is the process of taking a pretrained AI model and further training it on a smaller, specialized dataset tailored to a specific task or application.</p> <p> </p> <pre><code># 0) setup your dataset\ncurl -L https://huggingface.co/datasets/ksaw008/finance_alpaca/resolve/main/finance_alpaca.json -o my_custom_dataset.json\n\n# 1) Finetune a model (auto downloads weights)\nlitgpt finetune microsoft/phi-2 \\\n  --data JSON \\\n  --data.json_path my_custom_dataset.json \\\n  --data.val_split_fraction 0.1 \\\n  --out_dir out/custom-model\n\n# 2) Test the model\nlitgpt chat out/custom-model/final\n\n# 3) Deploy the model\nlitgpt serve out/custom-model/final\n</code></pre> <p>Read the full finetuning docs</p> <p> </p>"},{"location":"#deploy-an-llm","title":"Deploy an LLM","text":"<p>Deploy a pretrained or finetune LLM to use it in real-world applications. Deploy, automatically sets up a web server that can be accessed by a website or app.</p> <pre><code># deploy an out-of-the-box LLM\nlitgpt serve microsoft/phi-2\n\n# deploy your own trained model\nlitgpt serve path/to/microsoft/phi-2/checkpoint\n</code></pre> Show code to query server:  \u00a0  Test the server in a separate terminal and integrate the model API into your AI product:  <pre><code># 3) Use the server (in a separate Python session)\nimport requests, json\nresponse = requests.post(\n    \"http://127.0.0.1:8000/predict\",\n    json={\"prompt\": \"Fix typos in the following sentence: Example input\"}\n)\nprint(response.json()[\"output\"])\n</code></pre> <p>Read the full deploy docs.</p> <p> </p>"},{"location":"#evaluate-an-llm","title":"Evaluate an LLM","text":"<p>Evaluate an LLM to test its performance on various tasks to see how well it understands and generates text. Simply put, we can evaluate things like how well would it do in college-level chemistry, coding, etc... (MMLU, Truthful QA, etc...)</p> <pre><code>litgpt evaluate microsoft/phi-2 --tasks 'truthfulqa_mc2,mmlu'\n</code></pre> <p>Read the full evaluation docs.</p> <p> </p>"},{"location":"#test-an-llm","title":"Test an LLM","text":"<p>Test how well the model works via an interactive chat. Use the <code>chat</code> command to chat, extract embeddings, etc...</p> <p>Here's an example showing how to use the Phi-2 LLM:</p> <pre><code>litgpt chat microsoft/phi-2\n\n&gt;&gt; Prompt: What do Llamas eat?\n</code></pre> Full code: <pre><code># 1) List all supported LLMs\nlitgpt download list\n\n# 2) Use a model (auto downloads weights)\nlitgpt chat microsoft/phi-2\n\n&gt;&gt; Prompt: What do Llamas eat?\n</code></pre>  The download of certain models requires an additional access token. You can read more about this in the [download](tutorials/download_model_weights.md#specific-models-and-access-tokens) documentation.   <p>Read the full chat docs.</p> <p> </p>"},{"location":"#pretrain-an-llm","title":"Pretrain an LLM","text":"<p>Pretraining is the process of teaching an AI model by exposing it to a large amount of data before it is fine-tuned for specific tasks.</p> Show code: <pre><code>mkdir -p custom_texts\ncurl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt\ncurl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt\n\n# 1) Download a tokenizer\nlitgpt download EleutherAI/pythia-160m \\\n  --tokenizer_only True\n\n# 2) Pretrain the model\nlitgpt pretrain EleutherAI/pythia-160m \\\n  --tokenizer_dir EleutherAI/pythia-160m \\\n  --data TextFiles \\\n  --data.train_data_path \"custom_texts/\" \\\n  --train.max_tokens 10_000_000 \\\n  --out_dir out/custom-model\n\n# 3) Test the model\nlitgpt chat out/custom-model/final\n</code></pre> <p>Read the full pretraining docs</p> <p> </p>"},{"location":"#continue-pretraining-an-llm","title":"Continue pretraining an LLM","text":"<p>Continued pretraining is another way of finetuning that specializes an already pretrained model by training on custom data:</p> Show code: <pre><code>mkdir -p custom_texts\ncurl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt\ncurl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt\n\n# 1) Continue pretraining a model (auto downloads weights)\nlitgpt pretrain EleutherAI/pythia-160m \\\n  --tokenizer_dir EleutherAI/pythia-160m \\\n  --initial_checkpoint_dir EleutherAI/pythia-160m \\\n  --data TextFiles \\\n  --data.train_data_path \"custom_texts/\" \\\n  --train.max_tokens 10_000_000 \\\n  --out_dir out/custom-model\n\n# 2) Test the model\nlitgpt chat out/custom-model/final\n</code></pre> <p>Read the full continued pretraining docs</p> <p> </p>"},{"location":"#state-of-the-art-features","title":"State-of-the-art features","text":"<p>\u2705 \u00a0State-of-the-art optimizations: Flash Attention v2, multi-GPU support via fully-sharded data parallelism, optional CPU offloading, and TPU and XLA support.</p> <p>\u2705 \u00a0Pretrain, finetune, and deploy</p> <p>\u2705 \u00a0Reduce compute requirements with low-precision settings: FP16, BF16, and FP16/FP32 mixed.</p> <p>\u2705 \u00a0Lower memory requirements with quantization: 4-bit floats, 8-bit integers, and double quantization.</p> <p>\u2705 \u00a0Configuration files for great out-of-the-box performance.</p> <p>\u2705 \u00a0Parameter-efficient finetuning: LoRA, QLoRA, Adapter, and Adapter v2.</p> <p>\u2705 \u00a0Exporting to other popular model weight formats.</p> <p>\u2705 \u00a0Many popular datasets for pretraining and finetuning, and support for custom datasets.</p> <p>\u2705 \u00a0Readable and easy-to-modify code to experiment with the latest research ideas.</p> <p> </p>"},{"location":"#training-recipes","title":"Training recipes","text":"<p>LitGPT comes with validated recipes (YAML configs) to train models under different conditions. We've generated these recipes based on the parameters we found to perform the best for different training conditions.</p> <p>Browse all training recipes here.</p>"},{"location":"#example","title":"Example","text":"<pre><code>litgpt finetune \\\n  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml\n</code></pre> \u2705 Use configs to customize training  Configs let you customize training for all granular parameters like:  <pre><code># The path to the base model's checkpoint directory to load for finetuning. (type: &lt;class 'Path'&gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)\ncheckpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf\n\n# Directory in which to save checkpoints and logs. (type: &lt;class 'Path'&gt;, default: out/lora)\nout_dir: out/finetune/qlora-llama2-7b\n\n# The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\". (type: Optional[str], default: null)\nprecision: bf16-true\n\n...\n</code></pre> \u2705 Example: LoRA finetuning config <pre><code># The path to the base model's checkpoint directory to load for finetuning. (type: &lt;class 'Path'&gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)\ncheckpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf\n\n# Directory in which to save checkpoints and logs. (type: &lt;class 'Path'&gt;, default: out/lora)\nout_dir: out/finetune/qlora-llama2-7b\n\n# The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\". (type: Optional[str], default: null)\nprecision: bf16-true\n\n# If set, quantize the model with this algorithm. See ``tutorials/quantize.md`` for more information. (type: Optional[Literal['nf4', 'nf4-dq', 'fp4', 'fp4-dq', 'int8-training']], default: null)\nquantize: bnb.nf4\n\n# How many devices/GPUs to use. (type: Union[int, str], default: 1)\ndevices: 1\n\n# How many nodes to use. (type: int, default: 1)\nnum_nodes: 1\n\n# The LoRA rank. (type: int, default: 8)\nlora_r: 32\n\n# The LoRA alpha. (type: int, default: 16)\nlora_alpha: 16\n\n# The LoRA dropout value. (type: float, default: 0.05)\nlora_dropout: 0.05\n\n# Whether to apply LoRA to the query weights in attention. (type: bool, default: True)\nlora_query: true\n\n# Whether to apply LoRA to the key weights in attention. (type: bool, default: False)\nlora_key: false\n\n# Whether to apply LoRA to the value weights in attention. (type: bool, default: True)\nlora_value: true\n\n# Whether to apply LoRA to the output projection in the attention block. (type: bool, default: False)\nlora_projection: false\n\n# Whether to apply LoRA to the weights of the MLP in the attention block. (type: bool, default: False)\nlora_mlp: false\n\n# Whether to apply LoRA to output head in GPT. (type: bool, default: False)\nlora_head: false\n\n# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.\ndata:\n  class_path: litgpt.data.Alpaca2k\n  init_args:\n    mask_prompt: false\n    val_split_fraction: 0.05\n    prompt_style: alpaca\n    ignore_index: -100\n    seed: 42\n    num_workers: 4\n    download_dir: data/alpaca2k\n\n# Training-related arguments. See ``litgpt.args.TrainArgs`` for details\ntrain:\n\n  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)\n  save_interval: 200\n\n  # Number of iterations between logging calls (type: int, default: 1)\n  log_interval: 1\n\n  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 128)\n  global_batch_size: 8\n\n  # Number of samples per data-parallel rank (type: int, default: 4)\n  micro_batch_size: 2\n\n  # Number of iterations with learning rate warmup active (type: int, default: 100)\n  lr_warmup_steps: 10\n\n  # Number of epochs to train on (type: Optional[int], default: 5)\n  epochs: 4\n\n  # Total number of tokens to train on (type: Optional[int], default: null)\n  max_tokens:\n\n  # Limits the number of optimizer steps to run (type: Optional[int], default: null)\n  max_steps:\n\n  # Limits the length of samples (type: Optional[int], default: null)\n  max_seq_length: 512\n\n  # Whether to tie the embedding weights with the language modeling head weights (type: Optional[bool], default: null)\n  tie_embeddings:\n\n  #   (type: float, default: 0.0003)\n  learning_rate: 0.0002\n\n  #   (type: float, default: 0.02)\n  weight_decay: 0.0\n\n  #   (type: float, default: 0.9)\n  beta1: 0.9\n\n  #   (type: float, default: 0.95)\n  beta2: 0.95\n\n  #   (type: Optional[float], default: null)\n  max_norm:\n\n  #   (type: float, default: 6e-05)\n  min_lr: 6.0e-05\n\n# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details\neval:\n\n  # Number of optimizer steps between evaluation calls (type: int, default: 100)\n  interval: 100\n\n  # Number of tokens to generate (type: Optional[int], default: 100)\n  max_new_tokens: 100\n\n  # Number of iterations (type: int, default: 100)\n  max_iters: 100\n\n# The name of the logger to send metrics to. (type: Literal['wandb', 'tensorboard', 'csv'], default: csv)\nlogger_name: csv\n\n# The random seed to use for reproducibility. (type: int, default: 1337)\nseed: 1337\n</code></pre> \u2705 Override any parameter in the CLI: <pre><code>litgpt finetune \\\n  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml \\\n  --lora_r 4\n</code></pre>"},{"location":"#project-highlights","title":"Project highlights","text":"<p>LitGPT powers many great AI projects, initiatives, challenges and of course enterprises. Please submit a pull request to be considered for a feature.</p> \ud83d\udcca SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling  The [Samba](https://github.com/microsoft/Samba) project by researchers at Microsoft is built on top of the LitGPT code base and combines state space models with sliding window attention, which outperforms pure state space models.   \ud83c\udfc6 NeurIPS 2023 Large Language Model Efficiency Challenge: 1 LLM + 1 GPU + 1 Day  The LitGPT repository was the official starter kit for the [NeurIPS 2023 LLM Efficiency Challenge](https://llm-efficiency-challenge.github.io), which is a competition focused on finetuning an existing non-instruction tuned LLM for 24 hours on a single GPU.   \ud83e\udd99 TinyLlama: An Open-Source Small Language Model  LitGPT powered the [TinyLlama project](https://github.com/jzhang38/TinyLlama) and [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385) research paper.   \ud83c\udf6a MicroLlama: MicroLlama-300M  [MicroLlama](https://github.com/keeeeenw/MicroLlama) is a 300M Llama model pretrained on 50B tokens powered by TinyLlama and LitGPT.   \ud83d\udd2c Pre-training Small Base LMs with Fewer Tokens  The research paper [\"Pre-training Small Base LMs with Fewer Tokens\"](https://arxiv.org/abs/2404.08634), which utilizes LitGPT, develops smaller base language models by inheriting a few transformer blocks from larger models and training on a tiny fraction of the data used by the larger models. It demonstrates that these smaller models can perform comparably to larger models despite using significantly less training data and resources.   <p> </p>"},{"location":"#community","title":"Community","text":"<p>We welcome all individual contributors, regardless of their level of experience or hardware. Your contributions are valuable, and we are excited to see what you can accomplish in this collaborative and supportive environment.</p> <ul> <li>Request a feature</li> <li>Submit your first contribution</li> <li>Join our Discord</li> </ul> <p> </p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>\ud83d\ude80 Get started \u26a1\ufe0f Finetuning, incl. LoRA, QLoRA, and Adapters \ud83e\udd16 Pretraining \ud83d\udcac Model evaluation \ud83d\udcd8 Supported and custom datasets \ud83e\uddf9 Quantization \ud83e\udd2f Tips for dealing with out-of-memory (OOM) errors \ud83e\uddd1\ud83c\udffd\u200d\ud83d\udcbb Using cloud TPUs</p> <p> </p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This implementation extends on Lit-LLaMA and nanoGPT, and it's powered by Lightning Fabric \u26a1.</p> <ul> <li>@karpathy for nanoGPT</li> <li>@EleutherAI for GPT-NeoX and the Evaluation Harness</li> <li>@TimDettmers for bitsandbytes</li> <li>@Microsoft for LoRA</li> <li>@tridao for Flash Attention 2</li> </ul>"},{"location":"#license","title":"License","text":"<p>LitGPT is released under the Apache 2.0 license.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use LitGPT in your research, please cite the following work:</p> <pre><code>@misc{litgpt-2023,\n  author       = {Lightning AI},\n  title        = {LitGPT},\n  howpublished = {\\url{https://github.com/Lightning-AI/litgpt}},\n  year         = {2023},\n}\n</code></pre> <p> </p>"},{"location":"Reference/eval/","title":"Eval","text":""},{"location":"Reference/eval/#src.eval","title":"eval","text":"<p>Functions:</p> Name Description <code>generate_model_scores</code> <code>start_eval</code>"},{"location":"Reference/eval/#src.eval.console","title":"console","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"Reference/eval/#src.eval.generate_model_scores","title":"generate_model_scores","text":"<pre><code>generate_model_scores(\n    testing_datasets: list[dict[str, Any]],\n    eval_model: str = \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n    question_field: Literal[\"instruction\"] = \"instruction\",\n    target_field: Literal[\"output\"] = \"output\",\n    response_field: Literal[\"response\"] = \"response\",\n) -&gt; list[int]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>list[dict[str, Any]]</code> required <code>str</code> <code>'meta-llama/Llama-3.2-11B-Vision-Instruct'</code> <code>Literal['instruction']</code> <code>'instruction'</code> <code>Literal['output']</code> <code>'output'</code> <code>Literal['response']</code> <code>'response'</code> Source code in <code>src/eval.py</code> <pre><code>def generate_model_scores(\n    testing_datasets: list[dict[str, Any]],\n    eval_model: str = \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n    question_field: Literal[\"instruction\"] = \"instruction\",\n    target_field: Literal[\"output\"] = \"output\",\n    response_field: Literal[\"response\"] = \"response\",\n) -&gt; list[int]:\n    scores = []\n    client = InferenceClient()\n    with Progress() as progress:\n        task = progress.add_task(\"Scoring responses\", total=len(testing_datasets))\n        for testing_dataset in testing_datasets:\n            prompt = (\n                f\"Given the input `{testing_dataset[question_field]}` \"\n                f\"and correct output `{testing_dataset[target_field]}`, \"\n                f\"score the model response `{testing_dataset[response_field]}`\"\n                f\" on a scale from 0 to 100, where 100 is the best score. \"\n                f\"Respond with the integer number only.\"\n            )\n            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n            completion = client.chat.completions.create(\n                model=eval_model, messages=messages, max_tokens=500\n            )\n            score = completion.choices[0].message.content\n            try:\n                scores.append(int(score))\n            except ValueError:\n                continue\n            progress.update(task, advance=1, description=f\"Score: {score}\")\n    return scores\n</code></pre>"},{"location":"Reference/eval/#src.eval.generate_model_scores(testing_datasets)","title":"<code>testing_datasets</code>","text":""},{"location":"Reference/eval/#src.eval.generate_model_scores(eval_model)","title":"<code>eval_model</code>","text":""},{"location":"Reference/eval/#src.eval.generate_model_scores(question_field)","title":"<code>question_field</code>","text":""},{"location":"Reference/eval/#src.eval.generate_model_scores(target_field)","title":"<code>target_field</code>","text":""},{"location":"Reference/eval/#src.eval.generate_model_scores(response_field)","title":"<code>response_field</code>","text":""},{"location":"Reference/eval/#src.eval.start_eval","title":"start_eval","text":"<pre><code>start_eval(config: DictConfig) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> required Source code in <code>src/eval.py</code> <pre><code>@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"eval.yaml\")\ndef start_eval(config: DictConfig) -&gt; None:\n    console.print(OmegaConf.to_container(config))\n    prompt_style = Alpaca()\n    llm = litgpt.LLM.load(config.pretrained.model)\n\n    dataset = HFDataLoader(**config.data)\n    testing_datasets = dataset.load_list()\n    testing_datasets = [testing_datasets[0]]\n    with Progress() as progress:\n        task = progress.add_task(\"Generating responses\", total=len(testing_datasets))\n        for testing_dataset in testing_datasets:\n            prompt = prompt_style.apply(prompt=testing_dataset[\"instruction\"], **testing_dataset)\n            response = llm.generate(prompt=prompt)\n            testing_dataset[\"response\"] = response\n            progress.update(task, advance=1)\n    del llm\n    scores = generate_model_scores(testing_datasets=testing_datasets)\n    console.print(f\"Number of scores: {len(scores)} of {len(testing_datasets)}\")\n    console.print(f\"Average score: {sum(scores) / len(scores):.2f}\\n\")\n</code></pre>"},{"location":"Reference/eval/#src.eval.start_eval(config)","title":"<code>config</code>","text":""},{"location":"Reference/train/","title":"Train","text":""},{"location":"Reference/train/#src.train","title":"train","text":"<p>Functions:</p> Name Description <code>train</code>"},{"location":"Reference/train/#src.train.console","title":"console","text":"<pre><code>console = Console()\n</code></pre>"},{"location":"Reference/train/#src.train.train","title":"train","text":"<pre><code>train(config: DictConfig) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> required Source code in <code>src/train.py</code> <pre><code>@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"train.yaml\")\ndef train(config: DictConfig) -&gt; None:\n    console.print(OmegaConf.to_container(config))\n\n    callbacks = instantiate_callbacks(config.callbacks)\n\n    logger = instantiate_loggers(config.logger)\n\n    torch.set_float32_matmul_precision(\"medium\")\n    trainer = Trainer(**config.trainer, callbacks=callbacks, logger=logger)\n\n    with trainer.init_module(empty_init=True):\n        model = FinetuneLLM(**config.pretrained)\n\n    dataset = HFDataLoader(**config.data)\n    loaded_data = dataset.load_as_json()\n    loaded_data.connect(\n        tokenizer=model.tokenizer,\n        batch_size=config.data.batch_size,\n        max_seq_length=config.data.max_seq_length,\n    )\n\n    trainer.fit(model, loaded_data)\n\n    # Save final checkpoint\n    merge_lora_weights(model.model)\n    model_name = config.pretrained.model.split(\"/\")[-1]\n    ckpt_name = f\"{model_name}-finetuned.ckpt\"\n    trainer.save_checkpoint(f\"checkpoints/{ckpt_name}\", weights_only=True)\n</code></pre>"},{"location":"Reference/train/#src.train.train(config)","title":"<code>config</code>","text":""},{"location":"Reference/data/loader/","title":"Loader","text":""},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader","title":"HFDataLoader","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>load</code> <code>load_list</code> <code>load_as_json</code>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.path","title":"path","text":"<pre><code>path: str = Field(\n    ...,\n    title=\"The path to the dataset\",\n    description=\"The path/url to the dataset from huggingface.\",\n    frozen=True,\n    deprecated=False,\n)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.name","title":"name","text":"<pre><code>name: Optional[str] = Field(\n    default=None,\n    title=\"The name of the dataset\",\n    description=\"The name of the dataset from huggingface.\",\n    frozen=True,\n    deprecated=False,\n)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.split","title":"split","text":"<pre><code>split: str = Field(\n    \"train\",\n    title=\"The split of the dataset\",\n    description=\"The split of the dataset from huggingface.\",\n    frozen=True,\n    deprecated=False,\n)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.question","title":"question","text":"<pre><code>question: str = Field(\n    ...,\n    title=\"The Question of the dataset\",\n    description=\"This is the column name of the question, this field will be renamed to `input`.\",\n    frozen=True,\n    deprecated=False,\n)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.answer","title":"answer","text":"<pre><code>answer: str = Field(\n    ...,\n    title=\"The Answer of the dataset\",\n    description=\"This is the column name of the answer, this field will be renamed to `output`.\",\n    frozen=True,\n    deprecated=False,\n)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.max_cpu","title":"max_cpu","text":"<pre><code>max_cpu: int = Field(\n    default=8,\n    title=\"The maximum number of CPU cores\",\n    description=\"The maximum number of CPU cores to use for loading the dataset.\",\n    frozen=False,\n    deprecated=False,\n)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.load","title":"load","text":"<pre><code>load() -&gt; Dataset\n</code></pre> Source code in <code>src/data/loader.py</code> <pre><code>def load(self) -&gt; Dataset:\n    dataset = load_dataset(\n        cache_dir=\"./data/tmp\",\n        num_proc=self.max_cpu,\n        **self.model_dump(include={\"path\", \"name\", \"split\"}),\n    )\n    dataset = dataset.rename_columns({self.question: \"instruction\", self.answer: \"output\"})\n    need_to_remove = [\n        col for col in dataset.column_names if col not in [\"instruction\", \"output\"]\n    ]\n    dataset = dataset.remove_columns(need_to_remove)\n    return dataset\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.load_list","title":"load_list","text":"<pre><code>load_list() -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/data/loader.py</code> <pre><code>def load_list(self) -&gt; list[dict[str, Any]]:\n    dataset = self.load()\n    return dataset.to_list()\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.load_as_json","title":"load_as_json","text":"<pre><code>load_as_json(val_split: float = 0.2) -&gt; JSON\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>float</code> <code>0.2</code> Source code in <code>src/data/loader.py</code> <pre><code>def load_as_json(self, val_split: float = 0.2) -&gt; JSON:\n    dataset = self.load()\n    filepath = Path(dataset.cache_files[0][\"filename\"])\n    dataset_path = Path(f\"{filepath.parent}/{filepath.stem}.json\")\n    data = dataset.to_pandas()\n    data.to_json(\n        dataset_path.as_posix(), orient=\"records\", force_ascii=False, lines=False\n    )\n    return JSON(json_path=dataset_path, val_split_fraction=val_split)\n</code></pre>"},{"location":"Reference/data/loader/#src.data.loader.HFDataLoader.load_as_json(val_split)","title":"<code>val_split</code>","text":""},{"location":"Reference/model/lora/","title":"Lora","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM","title":"FinetuneLLM","text":"<pre><code>FinetuneLLM(\n    model: str,\n    lora_r: int,\n    lora_alpha: int,\n    lora_dropout: float,\n    lora_query: bool,\n    lora_key: bool,\n    lora_value: bool,\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>int</code> required <code>int</code> required <code>float</code> required <code>bool</code> required <code>bool</code> required <code>bool</code> required <p>Methods:</p> Name Description <code>setup</code> <code>on_train_start</code> <code>training_step</code> <code>compute_bleu_score</code> <code>configure_optimizers</code> Source code in <code>src/model/lora.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    lora_r: int,\n    lora_alpha: int,\n    lora_dropout: float,\n    lora_query: bool,\n    lora_key: bool,\n    lora_value: bool,\n):\n    super().__init__()\n    self.model_name = model\n    self.llm = litgpt.LLM.load(self.model_name)\n    self.model = GPT.from_name(\n        name=model,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        lora_query=lora_query,\n        lora_key=lora_key,\n        lora_value=lora_value,\n    )\n    self.tokenizer = litgpt.Tokenizer(f\"checkpoints/{self.model_name}\")\n    mark_only_lora_as_trainable(self.model)\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(model)","title":"<code>model</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(lora_r)","title":"<code>lora_r</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(lora_alpha)","title":"<code>lora_alpha</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(lora_dropout)","title":"<code>lora_dropout</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(lora_query)","title":"<code>lora_query</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(lora_key)","title":"<code>lora_key</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM(lora_value)","title":"<code>lora_value</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.llm","title":"llm","text":"<pre><code>llm = load(model_name)\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.model","title":"model","text":"<pre><code>model = from_name(\n    name=model,\n    lora_r=lora_r,\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    lora_query=lora_query,\n    lora_key=lora_key,\n    lora_value=lora_value,\n)\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.tokenizer","title":"tokenizer","text":"<pre><code>tokenizer = Tokenizer(f'checkpoints/{model_name}')\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.setup","title":"setup","text":"<pre><code>setup(stage: str) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/model/lora.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    pass\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.setup(stage)","title":"<code>stage</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.on_train_start","title":"on_train_start","text":"<pre><code>on_train_start() -&gt; None\n</code></pre> Source code in <code>src/model/lora.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    ckpt_path = f\"checkpoints/{self.model_name}/lit_model.pth\"\n    state_dict = torch.load(ckpt_path, mmap=True, weights_only=False)\n    self.model.load_state_dict(state_dict, strict=False)\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.training_step","title":"training_step","text":"<pre><code>training_step(batch: dict[str, torch.Tensor]) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dict[str, Tensor]</code> required Source code in <code>src/model/lora.py</code> <pre><code>def training_step(self, batch: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n    input_ids, targets = batch[\"input_ids\"], batch[\"labels\"]\n    logits = self.model(input_ids)\n    loss = chunked_cross_entropy(logits[..., :-1, :], targets[..., 1:])\n    self.log(\"train_loss\", loss, prog_bar=True)\n    return loss\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.training_step(batch)","title":"<code>batch</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.compute_bleu_score","title":"compute_bleu_score","text":"<pre><code>compute_bleu_score(generated_text: str, target_text: str) -&gt; float\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>str</code> required Source code in <code>src/model/lora.py</code> <pre><code>def compute_bleu_score(self, generated_text: str, target_text: str) -&gt; float:\n    self.log(\"generated_text\", generated_text)\n    self.log(\"target_text\", target_text)\n    reference = [target_text.split()]\n    candidate = generated_text.split()\n    score = sentence_bleu(reference, candidate)\n    return score\n</code></pre>"},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.compute_bleu_score(generated_text)","title":"<code>generated_text</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.compute_bleu_score(target_text)","title":"<code>target_text</code>","text":""},{"location":"Reference/model/lora/#src.model.lora.FinetuneLLM.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers() -&gt; (\n    tuple[list[torch.optim.Optimizer], list[torch.optim.lr_scheduler.LambdaLR]]\n)\n</code></pre> Source code in <code>src/model/lora.py</code> <pre><code>def configure_optimizers(\n    self,\n) -&gt; tuple[list[torch.optim.Optimizer], list[torch.optim.lr_scheduler.LambdaLR]]:\n    warmup_steps = 10\n    optimizer = torch.optim.AdamW(\n        self.model.parameters(), lr=0.0002, weight_decay=0.0, betas=(0.9, 0.95)\n    )\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: step / warmup_steps)\n    return [optimizer], [scheduler]\n</code></pre>"},{"location":"Reference/utils/instantiators/","title":"Instantiators","text":""},{"location":"Reference/utils/instantiators/#src.utils.instantiators","title":"instantiators","text":"<p>Functions:</p> Name Description <code>instantiate_callbacks</code> <p>Instantiates callbacks from config.</p> <code>instantiate_loggers</code> <p>Instantiates loggers from config.</p>"},{"location":"Reference/utils/instantiators/#src.utils.instantiators.instantiate_callbacks","title":"instantiate_callbacks","text":"<pre><code>instantiate_callbacks(callbacks_cfg: DictConfig) -&gt; list[Callback]\n</code></pre> <p>Instantiates callbacks from config.</p> <p>:param callbacks_cfg: A DictConfig object containing callback configurations. :return: A list of instantiated callbacks.</p> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> required Source code in <code>src/utils/instantiators.py</code> <pre><code>def instantiate_callbacks(callbacks_cfg: DictConfig) -&gt; list[Callback]:\n    \"\"\"Instantiates callbacks from config.\n\n    :param callbacks_cfg: A DictConfig object containing callback configurations.\n    :return: A list of instantiated callbacks.\n    \"\"\"\n    callbacks: list[Callback] = []\n\n    if not callbacks_cfg:\n        return callbacks\n\n    if not isinstance(callbacks_cfg, DictConfig):\n        raise TypeError(\"Callbacks config must be a DictConfig!\")\n\n    for cb_conf in callbacks_cfg.values():\n        if isinstance(cb_conf, DictConfig) and \"_target_\" in cb_conf:\n            callbacks.append(hydra.utils.instantiate(cb_conf))\n\n    return callbacks\n</code></pre>"},{"location":"Reference/utils/instantiators/#src.utils.instantiators.instantiate_callbacks(callbacks_cfg)","title":"<code>callbacks_cfg</code>","text":""},{"location":"Reference/utils/instantiators/#src.utils.instantiators.instantiate_loggers","title":"instantiate_loggers","text":"<pre><code>instantiate_loggers(logger_cfg: DictConfig) -&gt; list[Logger]\n</code></pre> <p>Instantiates loggers from config.</p> <p>:param logger_cfg: A DictConfig object containing logger configurations. :return: A list of instantiated loggers.</p> <p>Parameters:</p> Name Type Description Default <code>DictConfig</code> required Source code in <code>src/utils/instantiators.py</code> <pre><code>def instantiate_loggers(logger_cfg: DictConfig) -&gt; list[Logger]:\n    \"\"\"Instantiates loggers from config.\n\n    :param logger_cfg: A DictConfig object containing logger configurations.\n    :return: A list of instantiated loggers.\n    \"\"\"\n    logger: list[Logger] = []\n\n    if not logger_cfg:\n        return logger\n\n    if not isinstance(logger_cfg, DictConfig):\n        raise TypeError(\"Logger config must be a DictConfig!\")\n\n    for lg_conf in logger_cfg.values():\n        if isinstance(lg_conf, DictConfig) and \"_target_\" in lg_conf:\n            logger.append(hydra.utils.instantiate(lg_conf))\n\n    return logger\n</code></pre>"},{"location":"Reference/utils/instantiators/#src.utils.instantiators.instantiate_loggers(logger_cfg)","title":"<code>logger_cfg</code>","text":""},{"location":"Scripts/gen_docs/","title":"Gen docs","text":""},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator","title":"DocsGenerator","text":"<p>               Bases: <code>BaseModel</code></p> <p>DocsGenerator is a class that generates documentation for Python files or classes within a specified source directory.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source directory or file path.</p> <code>output</code> <code>str</code> <p>The output directory path.</p> <code>exclude</code> <code>str</code> <p>Comma-separated list of folders or files to exclude.</p> <code>mode</code> <code>Literal['file', 'class']</code> <p>Mode of documentation generation, either by file or class.</p> <p>Methods:</p> Name Description <code>gen_docs</code> <p>Generates documentation by file or class.</p> <code>__call__</code> <p>Asynchronously calls the gen_docs method.</p> Using CLI <pre><code>python ./scripts/gen_docs.py --source ./src --output ./docs/Reference --exclude .venv gen_docs\n</code></pre> Using Rye <pre><code>rye run gen\n</code></pre> <p>Methods:</p> Name Description <code>gen_docs</code> <p>This function can generate docs by file or class.</p>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.source","title":"source","text":"<pre><code>source: str = Field(..., frozen=True)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.output","title":"output","text":"<pre><code>output: str = Field(..., frozen=True)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.exclude","title":"exclude","text":"<pre><code>exclude: str = Field(\n    default=\".venv\",\n    description=\"Exclude the folder or file, it should be separated by comma.\",\n    examples=[\".venv,.git,.idea\"],\n)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.mode","title":"mode","text":"<pre><code>mode: Literal[\"file\", \"class\"] = Field(\n    default=\"class\", description=\"Generate docs by file or class.\"\n)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.gen_docs","title":"gen_docs","text":"<pre><code>gen_docs() -&gt; None\n</code></pre> <p>This function can generate docs by file or class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source path is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; pair_list = {\"./src\": \"./docs/Reference\"}\n&gt;&gt;&gt; for key, value in pair_list.items():\n...     docs_generator = DocsGenerator(source=key, output=value, exclude=\".venv\", mode=\"class\")\n...     asyncio.run(docs_generator.gen_docs())\n</code></pre> Source code in <code>scripts/gen_docs.py</code> <pre><code>async def gen_docs(self) -&gt; None:\n    \"\"\"This function can generate docs by file or class.\n\n    Raises:\n        ValueError: If the source path is invalid.\n\n    Examples:\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; pair_list = {\"./src\": \"./docs/Reference\"}\n        &gt;&gt;&gt; for key, value in pair_list.items():\n        ...     docs_generator = DocsGenerator(source=key, output=value, exclude=\".venv\", mode=\"class\")\n        ...     asyncio.run(docs_generator.gen_docs())\n    \"\"\"\n    with Progress() as progress:\n        task = progress.add_task(\"[green]Generating docs...\")\n        if self._source_path.is_dir():\n            await self.__remove_existing_folder()\n\n            need_to_exclude = [*self.exclude.split(\",\"), \"__init__.py\"]\n            files = self._source_path.glob(\"**/*.py\")\n            all_files = [\n                file for file in files if not any(f in file.parts for f in need_to_exclude)\n            ]\n\n            progress.update(\n                task_id=task, description=\"[cyan]Files Found...\", total=len(all_files)\n            )\n\n            for file in all_files:\n                docs_path = Path(\n                    f\"{self._output_path}/{file.parent.relative_to(self._source_path)}\"\n                )\n                processed_file = await self.__gen_single_docs(docs_path=docs_path, file=file)\n                progress.update(\n                    task_id=task,\n                    advance=1,\n                    description=f\"[cyan]Processing {processed_file}...\",\n                    refresh=True,\n                )\n\n        elif self._source_path.is_file():\n            progress.update(task_id=task, description=\"[cyan]Files Found...\", total=1)\n            processed_file = await self.__gen_single_docs(self._output_path, self._source_path)\n            progress.update(\n                task_id=task,\n                advance=1,\n                description=f\"[cyan]Processing {processed_file}...\",\n                refresh=True,\n            )\n        else:\n            raise ValueError(\"Invalid source path\")\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/10/06/this-is-an-example-post-for-blog/","title":"This is an example post for blog","text":"<p>This is an simple example post for blog.</p>"},{"location":"installation/pip/","title":"Using PIP","text":"<pre><code># clone project\ngit clone https://github.com/Mai0313/finetune\nmv finetune your-repo-name\n\n# change directory\ncd your-repo-name\n\n# [OPTIONAL] create conda environment\nconda create -n myenv python=3.9\nconda activate myenv\n\n# install requirements\npip install -r requirements.lock\n</code></pre>"},{"location":"installation/rye/","title":"Using Rye","text":""},{"location":"installation/rye/#step-1-install-rye","title":"Step 1: Install Rye","text":"<ul> <li>Visit the Rye Installation for installation.</li> </ul>"},{"location":"installation/rye/#step-2-clone-the-repository","title":"Step 2: Clone the repository","text":"<pre><code># clone project\ngit clone https://github.com/Mai0313/finetune\nmv finetune your-repo-name\n\n# change directory\ncd your-repo-name\n</code></pre>"},{"location":"installation/rye/#step-3-install-requirements","title":"Step 3: Install requirements","text":"<pre><code>rye sync\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}